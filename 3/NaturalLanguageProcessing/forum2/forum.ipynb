{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioGPT for Clinical Text Analysis - Jupyter Notebook\n",
    "\n",
    "## 1. Introduction to BioGPT\n",
    "- Developed by Microsoft Research\n",
    "- Domain-specific GPT variant for biomedical text\n",
    "- Capabilities:\n",
    "  - Medical text generation\n",
    "  - Clinical question answering\n",
    "  - Literature summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Environment\n",
    "First install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers torch datasets matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install protobuf torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Inference Example\n",
    "### 3.1 Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BioGptTokenizer, BioGptForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"microsoft/biogpt\"\n",
    "tokenizer = BioGptTokenizer.from_pretrained(model_name)\n",
    "model = BioGptForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Medical Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_medical_text(prompt, max_length=150):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k:v.cuda() for k,v in inputs.items()}\n",
    "        \n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=max_length,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example 1: Treatment Question\n",
    "print(generate_medical_text(\"The first-line treatment for hypertension involves\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Output:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The first-line treatment for hypertension involves lifestyle modifications such as weight reduction, dietary sodium restriction, and regular physical activity. When pharmacological treatment is required, thiazide diuretics, ACE inhibitors, or calcium channel blockers are typically recommended..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clinical QA Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "medical_qa = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "question = \"\"\"\n",
    "Question: What are the diagnostic criteria for type 2 diabetes?\n",
    "Context: Recent guidelines suggest that...\n",
    "\"\"\"\n",
    "\n",
    "result = medical_qa(\n",
    "    question,\n",
    "    max_length=300,\n",
    "    do_sample=True,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Architecture Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "# Create a dummy input\n",
    "dummy_input = tokenizer(\"Sample text\", return_tensors=\"pt\")['input_ids']\n",
    "\n",
    "# Visualize computation graph\n",
    "if torch.cuda.is_available():\n",
    "    dummy_input = dummy_input.cuda()\n",
    "\n",
    "outputs = model(dummy_input)\n",
    "make_dot(outputs.logits.mean(), params=dict(model.named_parameters())).render(\"biogpt_arch\", format=\"png\")\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename='biogpt_arch.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ethical Considerations\n",
    "\n",
    "- **Hallucination Risk**: Always verify outputs with medical professionals\n",
    "- **Data Privacy**: Never input real patient data\n",
    "- **Bias**: Models may reflect biases in training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "1. Fine-tune on specific medical domains\n",
    "2. Implement safety guardrails\n",
    "3. Combine with retrieval systems for fact-checking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
