{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44a13e90",
   "metadata": {},
   "source": [
    "# LSTM Model Training\n",
    "Below we will train a LSTM network using the training data and validate it using the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e709c",
   "metadata": {},
   "source": [
    "## Set enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b74412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        \u001b[38;2;0;0;0m▄\u001b[38;2;0;0;0m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;0;0;0m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;0;0;0m▄              \n",
      "      \u001b[38;2;0;0;0m▄\u001b[38;2;0;0;0m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;65;65;65m\u001b[48;2;65;65;65m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;65;65;65m\u001b[48;2;65;65;65m▀\u001b[0m\u001b[38;2;0;0;0m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;0;0;0m▄            \n",
      "      \u001b[38;2;0;0;0m▀\u001b[38;2;189;148;115m\u001b[48;2;0;0;0m▀\u001b[0m\u001b[38;2;65;65;65m\u001b[48;2;65;65;65m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;156;115;82m\u001b[48;2;65;65;65m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;0;0;0m▀\u001b[0m\u001b[38;2;0;0;0m▀      \u001b[38;2;0;0;0m▄\u001b[38;2;0;0;0m▄\u001b[38;2;0;0;0m▄   \n",
      "     \u001b[38;2;0;0;0m▄\u001b[38;2;0;0;0m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;156;115;82m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;65;65;65m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;0;0;0m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;0;0;0m▄  \u001b[38;2;0;0;0m▄\u001b[38;2;0;0;0m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;0;0;0m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;247;247;173m▀\u001b[0m\u001b[38;2;0;0;0m\u001b[48;2;247;247;173m▀\u001b[0m\u001b[38;2;0;0;0m\u001b[48;2;247;247;173m▀\u001b[0m\u001b[38;2;0;0;0m▄\n",
      "\u001b[38;2;0;0;0m▄\u001b[38;2;0;0;0m\u001b[48;2;255;255;255m▀\u001b[0m\u001b[38;2;0;0;0m\u001b[48;2;255;255;255m▀\u001b[0m\u001b[38;2;0;0;0m▄ \u001b[38;2;0;0;0m▀\u001b[38;2;189;148;115m\u001b[48;2;0;0;0m▀\u001b[0m\u001b[38;2;65;65;65m\u001b[48;2;65;65;65m▀\u001b[0m\u001b[38;2;65;65;65m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;255;255;255m▀\u001b[0m\u001b[38;2;255;255;255m\u001b[48;2;0;0;0m▀\u001b[0m\u001b[38;2;156;115;82m\u001b[48;2;255;255;255m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;156;115;82m▀\u001b[0m\u001b[38;2;0;0;0m\u001b[48;2;0;0;0m▀\u001b[0m \u001b[38;2;0;0;0m▄\u001b[38;2;189;148;115m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;247;247;173m▀\u001b[0m\u001b[38;2;247;247;173m\u001b[48;2;247;247;173m▀\u001b[0m\u001b[38;2;247;247;173m\u001b[48;2;247;247;173m▀\u001b[0m\u001b[38;2;247;247;173m\u001b[48;2;165;148;66m▀\u001b[0m\u001b[38;2;247;247;173m\u001b[48;2;65;65;65m▀\u001b[0m\u001b[38;2;165;148;66m\u001b[48;2;0;0;0m▀\u001b[0m\u001b[38;2;0;0;0m▀\n",
      "\u001b[38;2;0;0;0m▀\u001b[38;2;255;255;255m\u001b[48;2;0;0;0m▀\u001b[0m\u001b[38;2;255;255;255m\u001b[48;2;255;255;255m▀\u001b[0m\u001b[38;2;0;0;0m\u001b[48;2;255;255;255m▀\u001b[0m\u001b[38;2;0;0;0m▄\u001b[38;2;0;0;0m▄\u001b[38;2;0;0;0m\u001b[48;2;247;214;49m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;247;214;49m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;165;148;66m▀\u001b[0m\u001b[38;2;255;255;255m\u001b[48;2;165;148;66m▀\u001b[0m\u001b[38;2;65;65;65m\u001b[48;2;255;255;255m▀\u001b[0m\u001b[38;2;255;255;255m\u001b[48;2;156;115;82m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;156;115;82m▀\u001b[0m\u001b[38;2;65;65;65m\u001b[48;2;65;65;65m▀\u001b[0m\u001b[38;2;0;0;0m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;0;0;0m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;189;148;115m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;247;247;173m▀\u001b[0m\u001b[38;2;247;247;173m\u001b[48;2;247;247;173m▀\u001b[0m\u001b[38;2;247;247;173m\u001b[48;2;247;247;173m▀\u001b[0m\u001b[38;2;165;148;66m\u001b[48;2;247;247;173m▀\u001b[0m\u001b[38;2;165;148;66m\u001b[48;2;247;247;173m▀\u001b[0m\u001b[38;2;247;247;173m\u001b[48;2;247;247;173m▀\u001b[0m\u001b[38;2;247;247;173m\u001b[48;2;247;247;173m▀\u001b[0m\u001b[38;2;0;0;0m\u001b[48;2;0;0;0m▀\u001b[0m \n",
      "  \u001b[38;2;0;0;0m\u001b[48;2;0;0;0m▀\u001b[0m\u001b[38;2;65;65;65m\u001b[48;2;247;214;49m▀\u001b[0m\u001b[38;2;247;214;49m\u001b[48;2;247;214;49m▀\u001b[0m\u001b[38;2;247;214;49m\u001b[48;2;247;214;49m▀\u001b[0m\u001b[38;2;247;214;49m\u001b[48;2;165;148;66m▀\u001b[0m\u001b[38;2;247;214;49m\u001b[48;2;165;148;66m▀\u001b[0m\u001b[38;2;247;214;49m\u001b[48;2;255;255;255m▀\u001b[0m\u001b[38;2;247;214;49m\u001b[48;2;255;255;255m▀\u001b[0m\u001b[38;2;165;148;66m\u001b[48;2;255;255;255m▀\u001b[0m\u001b[38;2;156;115;82m\u001b[48;2;65;65;65m▀\u001b[0m\u001b[38;2;156;115;82m\u001b[48;2;65;65;65m▀\u001b[0m\u001b[38;2;156;115;82m\u001b[48;2;247;247;173m▀\u001b[0m\u001b[38;2;189;148;115m\u001b[48;2;247;247;173m▀\u001b[0m\u001b[38;2;247;247;173m\u001b[48;2;247;247;173m▀\u001b[0m\u001b[38;2;247;247;173m\u001b[48;2;247;247;173m▀\u001b[0m\u001b[38;2;247;247;173m\u001b[48;2;247;247;173m▀\u001b[0m\u001b[38;2;247;247;173m\u001b[48;2;247;247;173m▀\u001b[0m\u001b[38;2;247;247;173m\u001b[48;2;165;148;66m▀\u001b[0m\u001b[38;2;247;247;173m\u001b[48;2;165;148;66m▀\u001b[0m\u001b[38;2;165;148;66m\u001b[48;2;247;247;173m▀\u001b[0m\u001b[38;2;165;148;66m\u001b[48;2;247;247;173m▀\u001b[0m\u001b[38;2;0;0;0m\u001b[48;2;0;0;0m▀\u001b[0m  \n",
      "   \u001b[38;2;0;0;0m▀\u001b[38;2;247;214;49m\u001b[48;2;0;0;0m▀\u001b[0m\u001b[38;2;165;148;66m\u001b[48;2;0;0;0m▀\u001b[0m\u001b[38;2;247;214;49m\u001b[48;2;0;0;0m▀\u001b[0m\u001b[38;2;0;0;0m▀\u001b[38;2;0;0;0m▀\u001b[38;2;255;255;255m\u001b[48;2;0;0;0m▀\u001b[0m\u001b[38;2;255;255;255m\u001b[48;2;0;0;0m▀\u001b[0m\u001b[38;2;148;247;74m\u001b[48;2;123;156;74m▀\u001b[0m\u001b[38;2;148;247;74m\u001b[48;2;156;206;74m▀\u001b[0m\u001b[38;2;65;65;65m\u001b[48;2;148;247;74m▀\u001b[0m\u001b[38;2;65;65;65m\u001b[48;2;148;247;74m▀\u001b[0m\u001b[38;2;65;65;65m\u001b[48;2;156;206;74m▀\u001b[0m\u001b[38;2;247;247;173m\u001b[48;2;65;65;65m▀\u001b[0m\u001b[38;2;247;247;173m\u001b[48;2;65;65;65m▀\u001b[0m\u001b[38;2;247;247;173m\u001b[48;2;0;0;0m▀\u001b[0m\u001b[38;2;247;247;173m\u001b[48;2;0;0;0m▀\u001b[0m\u001b[38;2;247;247;173m\u001b[48;2;0;0;0m▀\u001b[0m\u001b[38;2;0;0;0m▀\u001b[38;2;0;0;0m▀   \n",
      "         \u001b[38;2;0;0;0m\u001b[48;2;0;0;0m▀\u001b[0m\u001b[38;2;247;247;173m\u001b[48;2;65;65;65m▀\u001b[0m\u001b[38;2;65;65;65m\u001b[48;2;247;247;173m▀\u001b[0m\u001b[38;2;65;65;65m\u001b[48;2;247;247;173m▀\u001b[0m\u001b[38;2;156;206;74m\u001b[48;2;65;65;65m▀\u001b[0m\u001b[38;2;156;206;74m\u001b[48;2;65;65;65m▀\u001b[0m\u001b[38;2;123;156;74m\u001b[48;2;65;65;65m▀\u001b[0m\u001b[38;2;123;156;74m\u001b[48;2;65;65;65m▀\u001b[0m\u001b[38;2;206;197;123m\u001b[48;2;0;0;0m▀\u001b[0m\u001b[38;2;0;0;0m▀       \n",
      "        \u001b[38;2;0;0;0m▀\u001b[38;2;165;148;66m\u001b[48;2;0;0;0m▀\u001b[0m\u001b[38;2;165;148;66m\u001b[48;2;0;0;0m▀\u001b[0m\u001b[38;2;0;0;0m▀\u001b[38;2;206;197;123m\u001b[48;2;0;0;0m▀\u001b[0m\u001b[38;2;206;197;123m\u001b[48;2;0;0;0m▀\u001b[0m\u001b[38;2;206;197;123m\u001b[48;2;65;65;65m▀\u001b[0m\u001b[38;2;65;65;65m\u001b[48;2;247;214;49m▀\u001b[0m\u001b[38;2;65;65;65m\u001b[48;2;247;214;49m▀\u001b[0m\u001b[38;2;247;214;49m\u001b[48;2;247;214;49m▀\u001b[0m\u001b[38;2;0;0;0m\u001b[48;2;0;0;0m▀\u001b[0m       \n",
      "              \u001b[38;2;0;0;0m▀\u001b[38;2;247;214;49m\u001b[48;2;0;0;0m▀\u001b[0m\u001b[38;2;0;0;0m▀\u001b[38;2;247;214;49m\u001b[48;2;0;0;0m▀\u001b[0m\u001b[38;2;0;0;0m▀       \n",
      "\u001b[0mCollecting tensorflow\n",
      "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /home/xgen0/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from tensorflow) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Using cached protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting requests<3,>=2.21.0 (from tensorflow)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: setuptools in /home/xgen0/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/xgen0/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/xgen0/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.9.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/xgen0/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Downloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/xgen0/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0mm\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m966.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading fonttools-4.56.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.9.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hUsing cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hUsing cached termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (408 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, namex, libclang, flatbuffers, wrapt, wheel, urllib3, tzdata, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, pyparsing, protobuf, pillow, optree, opt-einsum, numpy, mdurl, MarkupSafe, markdown, kiwisolver, idna, grpcio, google-pasta, gast, fonttools, cycler, charset-normalizer, certifi, absl-py, werkzeug, requests, pandas, ml-dtypes, markdown-it-py, h5py, contourpy, astunparse, tensorboard, rich, matplotlib, seaborn, keras, tensorflow\n",
      "Successfully installed MarkupSafe-3.0.2 absl-py-2.1.0 astunparse-1.6.3 certifi-2025.1.31 charset-normalizer-3.4.1 contourpy-1.3.1 cycler-0.12.1 flatbuffers-25.2.10 fonttools-4.56.0 gast-0.6.0 google-pasta-0.2.0 grpcio-1.71.0 h5py-3.13.0 idna-3.10 keras-3.9.0 kiwisolver-1.4.8 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 matplotlib-3.10.1 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.0.8 numpy-2.1.3 opt-einsum-3.4.0 optree-0.14.1 pandas-2.2.3 pillow-11.1.0 protobuf-5.29.3 pyparsing-3.2.1 pytz-2025.1 requests-2.32.3 rich-13.9.4 seaborn-0.13.2 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.5.0 tzdata-2025.1 urllib3-2.3.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install tensorflow pandas numpy matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d01ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 17:19:24.172180: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-17 17:19:24.180578: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742228364.189488  113381 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742228364.192316  113381 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742228364.199770  113381 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742228364.199779  113381 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742228364.199781  113381 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742228364.199782  113381 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-17 17:19:24.202299: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for data handling, preprocessing, and modeling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# For text preprocessing and tokenization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# For building the model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# For model evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a132c8",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7c18c5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load DATA\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_df = \u001b[43mpd\u001b[49m.read_csv(\u001b[33m'\u001b[39m\u001b[33m./data/sent_train.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m test_df = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33m./data/sent_test.csv\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Load DATA\n",
    "train_df = pd.read_csv('./data/sent_train.csv')\n",
    "test_df = pd.read_csv('./data/sent_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e12d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data: check for null values, data distribution, etc.\n",
    "print(train_df.head())\n",
    "print(train_df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ff8400",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f8ee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text data\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # remove punctuation\n",
    "    return text\n",
    "\n",
    "train_df['clean_text'] = train_df['tweet'].apply(clean_text)\n",
    "test_df['clean_text'] = test_df['tweet'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c0777",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b744108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters for tokenization and padding\n",
    "max_vocab = 5000  # maximum number of words to consider\n",
    "max_length = 50   # maximum length of a tweet in terms of word count\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=max_vocab, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_df['clean_text'])\n",
    "\n",
    "# Convert text to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['clean_text'])\n",
    "valid_sequences = tokenizer.texts_to_sequences(test_df['clean_text'])\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "X_train = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "X_valid = pad_sequences(valid_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Prepare target labels\n",
    "y_train = pd.get_dummies(train_df['sentiment']).values\n",
    "y_valid = pd.get_dummies(test_df['sentiment']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2916b201",
   "metadata": {},
   "source": [
    "The tokenizer converts words to integers, and padding ensures each sequence is of uniform length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfaad56",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe41fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model hyperparameters\n",
    "embedding_dim = 64\n",
    "lstm_units = 64\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_vocab, output_dim=embedding_dim, input_length=max_length),\n",
    "    LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')  # 3 classes: Bearish, Bullish, Neutral\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14d9943",
   "metadata": {},
   "source": [
    "The network starts with an embedding layer, followed by an LSTM to capture sequential dependencies, and ends with dense layers to output probabilities over the 3 sentiment classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7679044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with validation\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,  # adjust epochs as necessary\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a11088",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded19d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(model.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model.history['loss'], label='Train Loss')\n",
    "plt.plot(model.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_loss, val_accuracy = model.evaluate(X_valid, y_valid, verbose=0)\n",
    "print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Generate predictions and print classification report\n",
    "y_pred = model.predict(X_valid)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_valid, axis=1)\n",
    "\n",
    "print(classification_report(y_true_labels, y_pred_labels, target_names=['Bearish', 'Bullish', 'Neutral']))\n",
    "\n",
    "# Optional: Display confusion matrix\n",
    "cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Bearish', 'Bullish', 'Neutral'], yticklabels=['Bearish', 'Bullish', 'Neutral'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
