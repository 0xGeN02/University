{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a4fec5",
   "metadata": {},
   "source": [
    "# A3C Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f120ef",
   "metadata": {},
   "source": [
    "## A3C Algorithm Demo with CartPole in Gymnasium\n",
    "\n",
    "Implementation of Asynchronous Advantage Actor-Critic (A3C) algorithm to solve CartPole-v1 environment.\n",
    "\n",
    "**Objective**: Balance the pole as long as possible while keeping the cart within boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6775a48a",
   "metadata": {},
   "source": [
    "### Lib imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe467979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gymnasium in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (1.1.1)\n",
      "Requirement already satisfied: torch in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (2.6.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (3.10.1)\n",
      "Collecting pygame\n",
      "  Using cached pygame-2.6.1-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\manue\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading pygame-2.6.1-cp313-cp313-win_amd64.whl (10.6 MB)\n",
      "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/10.6 MB 4.6 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.3/10.6 MB 4.0 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.6/10.6 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.7/10.6 MB 5.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.0/10.6 MB 5.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.5/10.6 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.6/10.6 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.1/10.6 MB 5.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.4/10.6 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.6/10.6 MB 5.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pygame\n",
      "Successfully installed pygame-2.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium torch numpy matplotlib pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa33e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04494b9",
   "metadata": {},
   "source": [
    "#### Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ea578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09787155",
   "metadata": {},
   "source": [
    "### Neural Network Architecture\n",
    "Actor-Critic network with:\n",
    "- Shared layers for feature extraction\n",
    "- Actor head for action policy\n",
    "- Critic head for state value estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eec549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Linear(hidden_size, action_dim)\n",
    "        self.critic = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        # Xavier initialization for stability\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "\n",
    "    def forward(self, state):\n",
    "        shared_out = self.shared(state)\n",
    "        action_probs = torch.softmax(self.actor(shared_out), dim=-1)\n",
    "        state_value = self.critic(shared_out).squeeze()\n",
    "        return action_probs, state_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4ebc0e",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1a11201",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "445857dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LR = 0.001\n",
    "GAMMA = 0.99\n",
    "ENTROPY_COEFF = 0.01\n",
    "EPISODES = 300\n",
    "\n",
    "# Initialize components\n",
    "policy = PolicyNetwork(state_dim, action_dim)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=LR)\n",
    "\n",
    "# Reward tracking\n",
    "episode_rewards = []\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c244c99a",
   "metadata": {},
   "source": [
    "### 5. Training Function\n",
    "Episode training process including:\n",
    "1. Experience collection\n",
    "2. Advantage and return calculations\n",
    "3. Model parameter updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1b2cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_episode():\n",
    "    state, _ = env.reset()\n",
    "    transitions = []\n",
    "    \n",
    "    # Experience collection\n",
    "    while True:\n",
    "        state_tensor = torch.FloatTensor(state)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_probs, value = policy(state_tensor)\n",
    "            \n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        transitions.append({\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'done': done,\n",
    "            'value': value\n",
    "        })\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Calculate discounted returns\n",
    "    rewards = [t['reward'] for t in transitions]\n",
    "    dones = [t['done'] for t in transitions]\n",
    "    \n",
    "    returns = []\n",
    "    R = 0\n",
    "    for r, done in zip(reversed(rewards), reversed(dones)):\n",
    "        R = r + GAMMA * R * (not done)\n",
    "        returns.insert(0, R)\n",
    "    \n",
    "    # Prepare training data\n",
    "    states = torch.FloatTensor(np.array([t['state'] for t in transitions]))\n",
    "    actions = torch.LongTensor([t['action'] for t in transitions])\n",
    "    returns = torch.FloatTensor(returns)\n",
    "    \n",
    "    # Calculate losses\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    action_probs, values = policy(states)\n",
    "    advantages = returns - values.detach()\n",
    "    \n",
    "    # Policy Loss\n",
    "    log_probs = torch.log(action_probs.gather(1, actions.unsqueeze(1)))\n",
    "    policy_loss = -(log_probs.squeeze() * advantages).mean()\n",
    "    \n",
    "    # Value Loss\n",
    "    value_loss = 0.5 * (returns - values).pow(2).mean()\n",
    "    \n",
    "    # Entropy regularization\n",
    "    entropy = -(action_probs * torch.log(action_probs)).sum(dim=1).mean()\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = policy_loss + value_loss - ENTROPY_COEFF * entropy\n",
    "    \n",
    "    # Backpropagation\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return sum(rewards), total_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613a946d",
   "metadata": {},
   "source": [
    "### 6. Model Training\n",
    "Training process with live progress visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b9ed2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300/300\n",
      "Last reward: 63.0\n",
      "Last 10 avg: 56.0\n"
     ]
    }
   ],
   "source": [
    "# Configure live plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax1 = plt.subplot(121)\n",
    "ax2 = plt.subplot(122)\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    reward, loss = train_episode()\n",
    "    episode_rewards.append(reward)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Update plots every 10 episodes\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Reward plot\n",
    "        ax1.clear()\n",
    "        ax1.plot(episode_rewards, label='Episode reward')\n",
    "        ax1.plot(np.convolve(episode_rewards, np.ones(10)/10, mode='valid'), \n",
    "                label='Moving average (10)')\n",
    "        ax1.set_title(\"Training Progress\")\n",
    "        ax1.set_xlabel(\"Episode\")\n",
    "        ax1.set_ylabel(\"Reward\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Loss plot\n",
    "        ax2.clear()\n",
    "        ax2.plot(losses)\n",
    "        ax2.set_title(\"Loss Evolution\")\n",
    "        ax2.set_xlabel(\"Update step\")\n",
    "        ax2.set_ylabel(\"Total loss\")\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Episode {episode+1}/{EPISODES}\")\n",
    "        print(f\"Last reward: {reward}\")\n",
    "        print(f\"Last 10 avg: {np.mean(episode_rewards[-10:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3473ac12",
   "metadata": {},
   "source": [
    "### 7. Trained Model Demonstration\n",
    "Visualization of learned behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2cd24be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1:\n",
      "  Duration: 69.0 frames\n",
      "  Failure\n",
      "----------------------------------------\n",
      "Next episode starting in 2 seconds...\n",
      "Episode 2:\n",
      "  Duration: 57.0 frames\n",
      "  Failure\n",
      "----------------------------------------\n",
      "Next episode starting in 2 seconds...\n",
      "Episode 3:\n",
      "  Duration: 39.0 frames\n",
      "  Failure\n",
      "----------------------------------------\n",
      "Next episode starting in 2 seconds...\n",
      "Episode 4:\n",
      "  Duration: 51.0 frames\n",
      "  Failure\n",
      "----------------------------------------\n",
      "Next episode starting in 2 seconds...\n",
      "Episode 5:\n",
      "  Duration: 51.0 frames\n",
      "  Failure\n",
      "----------------------------------------\n",
      "Demonstration completed. Environment closed.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def run_demonstration(num_episodes=3, fps=30):\n",
    "    \"\"\"Run demonstration with adjustable speed and multiple episodes\n",
    "    \n",
    "    Args:\n",
    "        num_episodes (int): Number of episodes to demonstrate\n",
    "        fps (int): Frames per second (controls animation speed)\n",
    "    \"\"\"\n",
    "    test_env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "    episode_counter = 0\n",
    "    frame_delay = 1.0 / fps  # Calculate delay between frames\n",
    "    \n",
    "    try:\n",
    "        while episode_counter < num_episodes:\n",
    "            state, _ = test_env.reset()\n",
    "            episode_reward = 0\n",
    "            terminated = truncated = False\n",
    "            \n",
    "            while not (terminated or truncated):\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Get action from policy\n",
    "                with torch.no_grad():\n",
    "                    action_probs, _ = policy(torch.FloatTensor(state))\n",
    "                action = torch.argmax(action_probs).item()\n",
    "                \n",
    "                # Step environment\n",
    "                state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                # Maintain frame rate\n",
    "                elapsed = time.time() - start_time\n",
    "                remaining_delay = frame_delay - elapsed\n",
    "                if remaining_delay > 0:\n",
    "                    time.sleep(remaining_delay)\n",
    "            \n",
    "            print(f\"Episode {episode_counter+1}:\")\n",
    "            print(f\"  Duration: {episode_reward} frames\")\n",
    "            print(f\"  {'Success' if episode_reward >= 195 else 'Failure'}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            episode_counter += 1\n",
    "            \n",
    "            # Pause between episodes\n",
    "            if episode_counter < num_episodes:\n",
    "                print(\"Next episode starting in 2 seconds...\")\n",
    "                time.sleep(2)\n",
    "    \n",
    "    finally:\n",
    "        test_env.close()\n",
    "        print(\"Demonstration completed. Environment closed.\")\n",
    "\n",
    "# Run demonstration with parameters\n",
    "run_demonstration(\n",
    "    num_episodes=5,  # Number of episodes to show\n",
    "    fps=25           # Animation speed (typical movie frame rate)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f04a9",
   "metadata": {},
   "source": [
    "### 8. Results Analysis\n",
    "- Maximum possible reward: 500\n",
    "- Model should achieve rewards > 400 after ~200 episodes\n",
    "- Loss should stabilize when model converges\n",
    "\n",
    "**Key Components:**\n",
    "- Advantage calculation reduces variance\n",
    "- Entropy regularization maintains exploration\n",
    "- Shared network features improve sample efficiency\n",
    "\n",
    "**References:**\n",
    "1. [Policy Gradient Algorithms - Lilian Weng](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/)\n",
    "2. Mnih et al. (2016). Asynchronous Methods for Deep Reinforcement Learning\n",
    "3. Schulman et al. (2015). Trust Region Policy Optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
