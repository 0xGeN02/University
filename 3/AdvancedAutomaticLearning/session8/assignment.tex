\documentclass[conference]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath, amssymb}
\usepackage{cite}

\begin{document}

\title{Summary of A3C and TRPO in Reinforcement Learning \\ (IEEE LaTeX Format)}
\author{\IEEEauthorblockN{Your Name}
\IEEEauthorblockA{Your Affiliation\\
Email: your.email@example.com}}
\maketitle

\begin{abstract}
This document presents a concise summary of the key mathematical functions and equations for two reinforcement learning algorithms: Asynchronous Advantage Actor-Critic (A3C) and Trust Region Policy Optimization (TRPO). The document is formatted in IEEE LaTeX style, so you can easily copy and paste the code into your project.
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}
This document provides a brief overview of two important reinforcement learning methods:
\begin{itemize}
    \item \textbf{A3C (Asynchronous Advantage Actor-Critic)}: An actor-critic method that leverages multiple parallel agents running on separate threads to asynchronously update a shared global network. Each agent interacts with its own copy of the environment, reducing sample correlation and improving exploration.
    \item \textbf{TRPO (Trust Region Policy Optimization)}: A policy gradient method that restricts policy updates by imposing a trust region constraint based on the Kullback-Leibler (KL) divergence. This constraint ensures that each update leads to a monotonic improvement in the policy.
\end{itemize}

\section{A3C}
\subsection{n-step Advantage Function}
The advantage function in A3C is defined as:
\begin{equation}
A(s_t, a_t; \theta, \theta_v) = \sum_{i=0}^{k-1} \gamma^i\, r_{t+i} + \gamma^k\, V(s_{t+k}; \theta_v) - V(s_t; \theta_v)
\end{equation}
where \( \gamma \) is the discount factor, \( r_{t+i} \) is the reward at time \( t+i \), \( V(s;\theta_v) \) is the value function approximation, and \( k \) is the number of steps.

\subsection{Policy Gradient Update}
The policy gradient update is computed as:
\begin{equation}
\nabla_{\theta} \log \pi(a_t \mid s_t; \theta)\, A(s_t, a_t; \theta, \theta_v)
\end{equation}

\subsection{Entropy Regularization Term}
To encourage exploration, an entropy regularization term is added to the policy loss:
\begin{equation}
\nabla_{\theta} H\big(\pi(\cdot \mid s_t; \theta)\big)
\end{equation}
where the entropy is defined by:
\begin{equation}
H\big(\pi(\cdot \mid s_t; \theta)\big) = -\sum_{a} \pi(a \mid s_t; \theta) \log \pi(a \mid s_t; \theta)
\end{equation}

\section{TRPO}
\subsection{Surrogate Objective Function}
The surrogate objective function is defined as:
\begin{equation}
L_{\pi_{\text{old}}}(\pi) = \eta(\pi_{\text{old}}) + \sum_{s} \rho_{\pi_{\text{old}}}(s) \sum_{a} \pi(a \mid s)\, A_{\pi_{\text{old}}}(s,a)
\end{equation}
where:
\begin{itemize}
    \item \( \eta(\pi_{\text{old}}) \) is the expected return under the old policy.
    \item \( \rho_{\pi_{\text{old}}}(s) \) is the (unnormalized) discounted visitation frequency of state \( s \) under the old policy.
    \item \( A_{\pi_{\text{old}}}(s,a) \) is the advantage computed with the old policy.
\end{itemize}

\subsection{Trust Region Constraint (KL-Divergence)}
TRPO constrains the update by enforcing a limit on the average KL divergence between the old and new policies:
\begin{equation}
\bar{D}_{KL}(\pi_{\text{old}}, \pi) = \mathbb{E}_{s\sim \rho_{\pi_{\text{old}}}}\left[D_{KL}\big(\pi_{\text{old}}(\cdot \mid s) \,\|\, \pi(\cdot \mid s)\big)\right] \leq \delta,
\end{equation}
where \( \delta \) is the trust region limit.

\subsection{Natural Policy Gradient Update}
The policy update is computed using a natural policy gradient step:
\begin{equation}
\theta_{\text{new}} = \theta_{\text{old}} + \sqrt{\frac{2\delta}{g^T H^{-1} g}}\, H^{-1} g,
\end{equation}
where:
\begin{itemize}
    \item \( g = \nabla_\theta L_{\pi_{\text{old}}}(\pi)\big|_{\theta=\theta_{\text{old}}} \) is the gradient of the surrogate objective evaluated at the old policy.
    \item \( H \) is the Fisher information matrix (approximating the Hessian of the KL divergence).
\end{itemize}

\section{Conclusion}
This document has presented the key equations and functions for the A3C and TRPO algorithms in reinforcement learning, formatted in IEEE LaTeX style. You can use this document as a basis for further study or implementation.

\bibliographystyle{IEEEtran}
\bibliography{references} % Create your references.bib file with your citations

\end{document}
